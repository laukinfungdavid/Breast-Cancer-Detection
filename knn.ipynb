{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a class that storing the distance and the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class distancePredictedResult:\n",
    "    def __init__(self, distance, predictedResult):\n",
    "        self.distance = distance\n",
    "        self.predictedResult = predictedResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is to read the dataset from a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "def loadDataset(filename, header):\n",
    "    df = pd.read_csv(filename)\n",
    "    #map the result 'M' and 'B' to 1 and 0 respectively\n",
    "    df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})\n",
    "    #convert all the values from string to float\n",
    "    df = df.astype(float)\n",
    "    #add header to the dataframe\n",
    "    df.columns = header\n",
    "    #rearrange the column diagnosis: move it to the end\n",
    "    df = df[[c for c in df if c not in ['diagnosis']] + ['diagnosis']]\n",
    "    #drop the 'id' column\n",
    "    df = df.drop('id', 1)\n",
    "    #data normalization\n",
    "    normalized_df = (df - df.min()) / (df.max() - df.min())\n",
    "    return normalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The followings are different way to split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly split the data (82%: training data, 28%: testing data)\n",
    "def splitDataset_random(df):\n",
    "    train, test = train_test_split(df, test_size=0.28)\n",
    "    return train, test\n",
    "\n",
    "#the first 469 rows are training set and the remaining 100 rows are testing set\n",
    "def splitDataset_1(df):\n",
    "    train, test = df[:469], df[469:]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is to find out the euclideanDistance of two instances i.e. two datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find out the euclideanDistance of two instances\n",
    "import math\n",
    "def euclideanDistance(instance1, instance2, length):\n",
    "    distance = 0\n",
    "    for x in range(length):\n",
    "        distance += pow(float(instance1[x])-float(instance2[x]), 2)\n",
    "    return math.sqrt(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The followings are to get a sorted list storing all the neighbors according to the distance.\n",
    "The first one is just simply get the sorted list.\n",
    "The second one is the get the sorted neighbors having distance smaller than the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the sorted neighbors\n",
    "def getNeighbors(trainingSet, predictData):\n",
    "    neighbors = []\n",
    "    length = len(trainingSet.iloc[0])-1\n",
    "    for index, row in trainingSet.iterrows():\n",
    "        tempDistance = euclideanDistance(predictData, row, length)\n",
    "        neighbors.append(distancePredictedResult(tempDistance, row[length]))\n",
    "    neighbors.sort(key=lambda x: x.distance)\n",
    "    return neighbors\n",
    "\n",
    "#get the sorted neighbors with threshold\n",
    "def getNeighbors_threshold(trainingSet, predictData):\n",
    "    neighbors = []\n",
    "    neighbors_distance_only = []\n",
    "    length = len(trainingSet.iloc[0])-1\n",
    "    for index, row in trainingSet.iterrows():\n",
    "        tempDistance = euclideanDistance(predictData, row, length)\n",
    "        neighbors.append(distancePredictedResult(tempDistance, row[length]))\n",
    "        neighbors_distance_only.append(tempDistance)\n",
    "    neighbors.sort(key=lambda x: x.distance)\n",
    "    threshold = np.mean(neighbors_distance_only) + np.std(neighbors_distance_only)\n",
    "    neighbors_threshold = []\n",
    "    for i in range(len(neighbors)):\n",
    "        if neighbors[i].distance < threshold:\n",
    "            neighbors_threshold.append(neighbors[i])\n",
    "    return neighbors_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The followings are two different prediction. One is without adding weights to the votes according to distance and the other one is with adding weights to the votes according to distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the prediction without adding weights to the votes according to distance\n",
    "import operator\n",
    "def getPrediction(dataPoints, k):\n",
    "    neighbors = []\n",
    "    for i in range(k):\n",
    "        neighbors.append(dataPoints[i])\n",
    "    votes = {}\n",
    "    for i in range(len(neighbors)):\n",
    "        classResult = neighbors[i].predictedResult\n",
    "        if classResult not in votes:\n",
    "            votes[classResult] = 1\n",
    "        else:\n",
    "            votes[classResult] += 1\n",
    "    sortedKeys = sorted(votes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedKeys[0][0]\n",
    "\n",
    "#get the prediction with adding weights to the votes according to distance\n",
    "def getPredictionWithWeight(dataPoints, k):\n",
    "    neighbors = []\n",
    "    for i in range(k):\n",
    "        neighbors.append(dataPoints[i])\n",
    "    votes = {}\n",
    "    for i in range(len(neighbors)):\n",
    "        classResult = neighbors[i].predictedResult\n",
    "        if classResult not in votes:\n",
    "            votes[classResult] = 1/neighbors[i].distance\n",
    "        else:\n",
    "            votes[classResult] += 1/neighbors[i].distance\n",
    "    sortedKeys = sorted(votes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedKeys[0][0]\n",
    "\n",
    "#get the prediction and if there is a tie, reduce k\n",
    "def getPrediction_dealing_tie(dataPoints, k):\n",
    "    found_result = False\n",
    "    while not found_result:\n",
    "        neighbors = []\n",
    "        for i in range(k):\n",
    "            neighbors.append(dataPoints[i])\n",
    "        votes = {}\n",
    "        for i in range(len(neighbors)):\n",
    "            classResult = neighbors[i].predictedResult\n",
    "            if classResult not in votes:\n",
    "                votes[classResult] = 1\n",
    "            else:\n",
    "                votes[classResult] += 1\n",
    "        sortedKeys = sorted(votes.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        if len(sortedKeys) > 1:\n",
    "            if sortedKeys[0][1] == sortedKeys[1][1]:\n",
    "                k -= 1\n",
    "            else:\n",
    "                return sortedKeys[0][0]\n",
    "        else:\n",
    "            return sortedKeys[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is to compare the predictions with the ground truth in order to get the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the accuracy by comparing the predictions with the ground truth\n",
    "def getAccuracy(testingSet, predictions, columnToBePredict):\n",
    "    votes = {'true':0, 'false':0}\n",
    "    for index, row in testingSet.iterrows():\n",
    "        if row[columnToBePredict] == predictions[index]:\n",
    "            votes['true'] += 1\n",
    "        #print('prediction: ', predictions[index], '     result: ', row[columnToBePredict])\n",
    "        \n",
    "    return (votes['true']/float(len(testingSet)))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are different experiments stated in the assignment sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the normal way to implement k-nn\n",
    "def normalAgorithm():\n",
    "    filename = 'wisc_bc_data.csv'\n",
    "    header = ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'points_mean', 'symmetry_mean', 'dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'points_se', 'symmetry_se', 'dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'points_worst', 'symmetry_worst', 'dimension_worst']\n",
    "    columnToBePredict = 'diagnosis'\n",
    "    normalized_df = loadDataset(filename, header)\n",
    "    trainingSet, testingSet = splitDataset_1(normalized_df)\n",
    "    predictions = {}\n",
    "    k = int(math.sqrt(len(trainingSet)))\n",
    "\n",
    "    for index, row in testingSet.iterrows():\n",
    "        tempNeighbors = getNeighbors(trainingSet, row)\n",
    "        predictions[index] = getPrediction(tempNeighbors, k)\n",
    "    \n",
    "    print('Accuracy when using the normal way to implement the algorithm: ', getAccuracy(testingSet, predictions, columnToBePredict), '%')\n",
    "\n",
    "#adding weights to the votes according to distance\n",
    "def normalAgorithm_adding_distance_weight():\n",
    "    filename = 'wisc_bc_data.csv'\n",
    "    header = ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'points_mean', 'symmetry_mean', 'dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'points_se', 'symmetry_se', 'dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'points_worst', 'symmetry_worst', 'dimension_worst']\n",
    "    columnToBePredict = 'diagnosis'\n",
    "    normalized_df = loadDataset(filename, header)\n",
    "    trainingSet, testingSet = splitDataset_1(normalized_df)\n",
    "    predictions = {}\n",
    "    k = int(math.sqrt(len(trainingSet)))\n",
    "\n",
    "    for index, row in testingSet.iterrows():\n",
    "        tempNeighbors = getNeighbors(trainingSet, row)\n",
    "        predictions[index] = getPredictionWithWeight(tempNeighbors, k)\n",
    "    \n",
    "    print('Accuracy when adding weight according to the distance: ', getAccuracy(testingSet, predictions, columnToBePredict), '%')\n",
    "    \n",
    "#try different k values\n",
    "#the following try the square root of the number of data of the training set, half of the training set, one third of the training set and one forth of the training set\n",
    "def different_k_Agorithm():\n",
    "    filename = 'wisc_bc_data.csv'\n",
    "    header = ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'points_mean', 'symmetry_mean', 'dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'points_se', 'symmetry_se', 'dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'points_worst', 'symmetry_worst', 'dimension_worst']\n",
    "    columnToBePredict = 'diagnosis'\n",
    "    normalized_df = loadDataset(filename, header)\n",
    "    trainingSet, testingSet = splitDataset_1(normalized_df)\n",
    "    predictions = {}\n",
    "    #try different k values\n",
    "    k = [int(math.sqrt(len(trainingSet))), int(len(trainingSet)/2), int(len(trainingSet)/3), int(len(trainingSet)/4)]\n",
    "    \n",
    "    for i in range(len(k)):\n",
    "        for index, row in testingSet.iterrows():\n",
    "            tempNeighbors = getNeighbors(trainingSet, row)\n",
    "            predictions[index] = getPrediction(tempNeighbors, k[i])\n",
    "        print('Accuracy when trying different k values (',k[i], '): ', getAccuracy(testingSet, predictions, columnToBePredict), '%')\n",
    "        \n",
    "#if there is a tie, reduce k until there is a clearer winner\n",
    "def algorithm_dealing_tie():\n",
    "    filename = 'wisc_bc_data.csv'\n",
    "    header = ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'points_mean', 'symmetry_mean', 'dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'points_se', 'symmetry_se', 'dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'points_worst', 'symmetry_worst', 'dimension_worst']\n",
    "    columnToBePredict = 'diagnosis'\n",
    "    normalized_df = loadDataset(filename, header)\n",
    "    trainingSet, testingSet = splitDataset_1(normalized_df)\n",
    "    predictions = {}\n",
    "    k = int(math.sqrt(len(trainingSet)))\n",
    "\n",
    "    for index, row in testingSet.iterrows():\n",
    "        tempNeighbors = getNeighbors(trainingSet, row)\n",
    "        predictions[index] = getPrediction_dealing_tie(tempNeighbors, k)\n",
    "    \n",
    "    print('Accuracy when using the normal way to implement the algorithm (handle tie): ', getAccuracy(testingSet, predictions, columnToBePredict), '%')\n",
    "    \n",
    "#split the data randomly using the normal algorithm\n",
    "def algorithm_random_split_data():\n",
    "    filename = 'wisc_bc_data.csv'\n",
    "    header = ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'points_mean', 'symmetry_mean', 'dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'points_se', 'symmetry_se', 'dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'points_worst', 'symmetry_worst', 'dimension_worst']\n",
    "    columnToBePredict = 'diagnosis'\n",
    "    normalized_df = loadDataset(filename, header)\n",
    "    trainingSet, testingSet = splitDataset_random(normalized_df)\n",
    "    predictions = {}\n",
    "    k = int(math.sqrt(len(trainingSet)))\n",
    "\n",
    "    for index, row in testingSet.iterrows():\n",
    "        tempNeighbors = getNeighbors(trainingSet, row)\n",
    "        predictions[index] = getPredictionWithWeight(tempNeighbors, k)\n",
    "    \n",
    "    print('Accuracy when spliting the data randomly: ', getAccuracy(testingSet, predictions, columnToBePredict), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    normalAgorithm()\n",
    "    normalAgorithm_adding_distance_weight()\n",
    "    different_k_Agorithm()\n",
    "    algorithm_dealing_tie()\n",
    "    algorithm_random_split_data() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy when using the normal way to implement the algorithm:  98.0 %\n",
      "Accuracy when adding weight according to the distance:  98.0 %\n",
      "Accuracy when trying different k values ( 21 ):  98.0 %\n",
      "Accuracy when trying different k values ( 234 ):  89.0 %\n",
      "Accuracy when trying different k values ( 156 ):  91.0 %\n",
      "Accuracy when trying different k values ( 117 ):  92.0 %\n",
      "Accuracy when using the normal way to implement the algorithm (handle tie):  98.0 %\n",
      "Accuracy when spliting the data randomly:  97.5 %\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From different experiments, I found out that by spliting data randomly and adding weight according to the distance can sometimes get an accuracy that is higher than 98% but it is not a must as sometimes it may be lower than 98%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
